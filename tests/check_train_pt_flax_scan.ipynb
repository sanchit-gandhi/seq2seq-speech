{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d2b88cb-29ba-4e6c-b997-fbda2bd79886",
   "metadata": {},
   "source": [
    "### 1. Set the JAX platform (CPU/TPU) and matmul precision (if on TPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf96bb2-c981-4992-bb87-484add73c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "#os.environ[\"JAX_DEFAULT_MATMUL_PRECISION\"]=\"float32\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26915c47-7e20-4a15-a98e-fd288e664a59",
   "metadata": {},
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26be603c-6705-4487-837b-680d6e4fae0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "I0000 00:00:1650545753.222788   84139 tpu_initializer_helper.cc:94] libtpu.so already in use by another process. Run \"$ sudo lsof -w /dev/accel0\" to figure out which process is using the TPU. Not attempting to load libtpu.so in this process.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoConfig, AutoModelForSpeechSeq2Seq, FlaxAutoModelForSpeechSeq2Seq, AutoFeatureExtractor, AutoTokenizer, AutoProcessor, FlaxSpeechEncoderDecoderModel, SpeechEncoderDecoderModel\n",
    "from models.modeling_flax_speech_encoder_decoder import FlaxSpeechEncoderDecoderModel as CustomFlaxSpeechEncoderDecoderModel\n",
    "import flax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from numpy.random import default_rng\n",
    "import tempfile\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "import torch\n",
    "from flax.training.common_utils import onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3223a0-5b21-41f6-9afd-ed6c3cae4e33",
   "metadata": {},
   "source": [
    "### 3. Set model, training and data args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a97514-11ac-4047-a426-418ee8cf84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model args\n",
    "tiny = False\n",
    "\n",
    "if tiny:\n",
    "    encoder_id = \"hf-internal-testing/tiny-random-wav2vec2\"\n",
    "    decoder_id = \"hf-internal-testing/tiny-random-bart\"\n",
    "\n",
    "else:\n",
    "    encoder_id = \"facebook/wav2vec2-large-lv60\"\n",
    "    decoder_id = \"patrickvonplaten/bart-large-fp32\"\n",
    "    \n",
    "# training args\n",
    "batch_size_per_update = 2\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# data args\n",
    "dataset_name = \"librispeech_asr\"\n",
    "dataset_config_name = \"clean\"\n",
    "train_split_name = \"train.100[:5%]\"\n",
    "eval_split_name = \"validation[:5%]\"\n",
    "dataset_cache_dir = \"/home/sanchitgandhi/cache/huggingface/datasets\"\n",
    "audio_column_name = \"audio\"\n",
    "text_column_name = \"text\"\n",
    "do_lower_case = True\n",
    "\n",
    "max_duration_in_seconds = 5\n",
    "min_duration_in_seconds = 0\n",
    "max_target_length = 32\n",
    "min_target_length = 0\n",
    "pad_input_to_multiple_of = 32000\n",
    "pad_target_to_multiple_of = None\n",
    "max_train_samples = max_eval_samples = None\n",
    "preprocessing_num_workers = num_workers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eac1f8-35e4-417f-abda-adc9ce7a73d3",
   "metadata": {},
   "source": [
    "### 4. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce7ceeb-e83f-4adf-a27d-1d3cecffe2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "raw_datasets[\"train\"] = load_dataset(\n",
    "            dataset_name,\n",
    "            dataset_config_name,\n",
    "            split=train_split_name,\n",
    "            cache_dir=dataset_cache_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68816f30-d890-4c83-ab9d-8313d6bddedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets[\"eval\"] = load_dataset(\n",
    "            dataset_name,\n",
    "            dataset_config_name,\n",
    "            split=eval_split_name,\n",
    "            cache_dir=dataset_cache_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452f55b-48ca-4fd7-a5c4-be538f66ec0d",
   "metadata": {},
   "source": [
    "### 5. Load pretrained model, tokenizer, and feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403e63e4-e1a6-4ad5-bf76-ffa797ca744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder checkpointing: False\n",
      "encoder scan: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1269579776 bytes == 0x62c98000 @  0x7f76e53be680 0x7f76e53df824 0x5f8a01 0x648cf1 0x5c4676 0x4f290e 0x64f718 0x5048b3 0x56b1da 0x56939a 0x5f6a13 0x50aa2c 0x5f3547 0x56c8cd 0x56939a 0x50aaa0 0x56c28c 0x56939a 0x68d047 0x6003a4 0x5c4a40 0x56b0ae 0x5002d8 0x56cadf 0x5002d8 0x56cadf 0x5002d8 0x503fb6 0x56b1da 0x5f6836 0x56b0ae\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-lv60 were not used when initializing FlaxWav2Vec2Model: {('encoder', 'layers', '21', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '2', 'layer_norm', 'scale'), ('encoder', 'layers', '3', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '16', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '20', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '8', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '8', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '7', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '7', 'final_layer_norm', 'bias'), ('encoder', 'layers', '10', 'layer_norm', 'scale'), ('encoder', 'layers', '7', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '9', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '9', 'final_layer_norm', 'bias'), ('encoder', 'layers', '19', 'final_layer_norm', 'scale'), ('encoder', 'layers', '11', 'layer_norm', 'bias'), ('encoder', 'layers', '22', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '21', 'final_layer_norm', 'scale'), ('encoder', 'layers', '13', 'layer_norm', 'bias'), ('encoder', 'layers', '0', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '19', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '16', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '11', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '1', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '23', 'final_layer_norm', 'bias'), ('encoder', 'layers', '13', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '4', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '18', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '3', 'final_layer_norm', 'bias'), ('encoder', 'layers', '20', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '14', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '9', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '9', 'final_layer_norm', 'scale'), ('encoder', 'layers', '13', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '11', 'layer_norm', 'scale'), ('encoder', 'layers', '13', 'layer_norm', 'scale'), ('encoder', 'layers', '10', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '23', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '13', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '19', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '12', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '11', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '23', 'final_layer_norm', 'scale'), ('encoder', 'layers', '17', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '22', 'final_layer_norm', 'bias'), ('encoder', 'layers', '5', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '3', 'final_layer_norm', 'scale'), ('encoder', 'layers', '2', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '8', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '12', 'final_layer_norm', 'bias'), ('encoder', 'layers', '16', 'layer_norm', 'bias'), ('encoder', 'layers', '21', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '19', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '16', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '20', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '5', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '8', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '8', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '4', 'layer_norm', 'bias'), ('encoder', 'layers', '11', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '4', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '1', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '7', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '12', 'final_layer_norm', 'scale'), ('encoder', 'layers', '22', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '16', 'layer_norm', 'scale'), ('encoder', 'layers', '6', 'final_layer_norm', 'scale'), ('encoder', 'layers', '6', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '11', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '14', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '2', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '4', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '13', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '18', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '4', 'layer_norm', 'scale'), ('encoder', 'layers', '15', 'layer_norm', 'bias'), ('encoder', 'layers', '3', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '20', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '10', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '23', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '15', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '14', 'final_layer_norm', 'bias'), ('encoder', 'layers', '18', 'layer_norm', 'bias'), ('encoder', 'layers', '7', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '6', 'layer_norm', 'bias'), ('encoder', 'layers', '12', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '7', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '12', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '17', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '0', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '0', 'layer_norm', 'scale'), ('encoder', 'layers', '5', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '15', 'layer_norm', 'scale'), ('encoder', 'layers', '2', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '14', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '15', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '18', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '12', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '19', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '5', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '14', 'final_layer_norm', 'scale'), ('encoder', 'layers', '18', 'layer_norm', 'scale'), ('encoder', 'layers', '6', 'layer_norm', 'scale'), ('encoder', 'layers', '10', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '14', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '13', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '6', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '2', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '4', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '17', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '7', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '20', 'layer_norm', 'scale'), ('encoder', 'layers', '22', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '15', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '17', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '6', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '8', 'layer_norm', 'bias'), ('encoder', 'layers', '5', 'final_layer_norm', 'bias'), ('encoder', 'layers', '8', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '14', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '2', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '14', 'layer_norm', 'bias'), ('encoder', 'layers', '8', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '7', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '13', 'final_layer_norm', 'scale'), ('encoder', 'layers', '19', 'final_layer_norm', 'bias'), ('encoder', 'layers', '3', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '7', 'layer_norm', 'bias'), ('encoder', 'layers', '15', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '23', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '5', 'final_layer_norm', 'scale'), ('encoder', 'layers', '8', 'layer_norm', 'scale'), ('encoder', 'layers', '7', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '9', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '7', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '12', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '0', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '22', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '19', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '6', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '23', 'layer_norm', 'bias'), ('encoder', 'layers', '4', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '17', 'layer_norm', 'bias'), ('encoder', 'layers', '14', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '18', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '11', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '1', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '8', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '20', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '22', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '10', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '14', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '13', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '6', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '18', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '2', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '20', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '22', 'layer_norm', 'bias'), ('encoder', 'layers', '17', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '23', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '23', 'layer_norm', 'scale'), ('encoder', 'layers', '16', 'final_layer_norm', 'bias'), ('encoder', 'layers', '17', 'layer_norm', 'scale'), ('encoder', 'layers', '7', 'final_layer_norm', 'scale'), ('encoder', 'layers', '11', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '1', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '22', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '15', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '21', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '12', 'layer_norm', 'bias'), ('encoder', 'layers', '17', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '5', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '19', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '16', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '21', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '19', 'layer_norm', 'bias'), ('encoder', 'layers', '23', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '21', 'layer_norm', 'bias'), ('encoder', 'layers', '8', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '0', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '7', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '21', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '16', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '5', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '2', 'final_layer_norm', 'bias'), ('encoder', 'layers', '12', 'layer_norm', 'scale'), ('encoder', 'layers', '23', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '9', 'layer_norm', 'bias'), ('encoder', 'layers', '9', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '19', 'layer_norm', 'scale'), ('encoder', 'layers', '9', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '21', 'layer_norm', 'scale'), ('encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('encoder', 'layers', '22', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '6', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '11', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '19', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '2', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '15', 'final_layer_norm', 'bias'), ('encoder', 'layers', '12', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '10', 'final_layer_norm', 'bias'), ('encoder', 'layers', '22', 'final_layer_norm', 'scale'), ('encoder', 'layers', '4', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '11', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '21', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '13', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '9', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '1', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '8', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '20', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '22', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '2', 'final_layer_norm', 'scale'), ('encoder', 'layers', '3', 'layer_norm', 'bias'), ('encoder', 'layers', '2', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '3', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '9', 'layer_norm', 'scale'), ('encoder', 'layers', '18', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '20', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '10', 'final_layer_norm', 'scale'), ('encoder', 'layers', '3', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '23', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '11', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '11', 'final_layer_norm', 'bias'), ('encoder', 'layers', '1', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '13', 'final_layer_norm', 'bias'), ('encoder', 'layers', '12', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '21', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '5', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '19', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '0', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '14', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '16', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '12', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '21', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '3', 'layer_norm', 'scale'), ('encoder', 'layers', '23', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '0', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '19', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '21', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '22', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('encoder', 'layers', '14', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '16', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '5', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '11', 'final_layer_norm', 'scale'), ('encoder', 'layers', '13', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '17', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '9', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '11', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '2', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '1', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '12', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '0', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '15', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '5', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'final_layer_norm', 'scale'), ('encoder', 'layers', '21', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '13', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '11', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '10', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '2', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '3', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '3', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '15', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '4', 'final_layer_norm', 'bias'), ('encoder', 'layers', '22', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '6', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '18', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '12', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '0', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '14', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '18', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '12', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '5', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '6', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '10', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '4', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '20', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '18', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '10', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '22', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '4', 'final_layer_norm', 'scale'), ('encoder', 'layers', '14', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '14', 'layer_norm', 'scale'), ('encoder', 'layers', '17', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '3', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '10', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '13', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '16', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '17', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '17', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '23', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '7', 'layer_norm', 'scale'), ('encoder', 'layers', '15', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '18', 'final_layer_norm', 'bias'), ('encoder', 'layers', '20', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '12', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '6', 'final_layer_norm', 'bias'), ('encoder', 'layers', '4', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '9', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '15', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '15', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '5', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '5', 'layer_norm', 'bias'), ('encoder', 'layers', '20', 'final_layer_norm', 'bias'), ('encoder', 'layers', '10', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '19', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '23', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '21', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '18', 'final_layer_norm', 'scale'), ('encoder', 'layers', '0', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '8', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '15', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '6', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '16', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'layer_norm', 'bias'), ('encoder', 'layers', '5', 'layer_norm', 'scale'), ('encoder', 'layers', '1', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '22', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '6', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '18', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '20', 'final_layer_norm', 'scale'), ('encoder', 'layers', '22', 'layer_norm', 'scale'), ('encoder', 'layers', '2', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '3', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '18', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '9', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '16', 'final_layer_norm', 'scale'), ('encoder', 'layers', '8', 'final_layer_norm', 'bias'), ('encoder', 'layers', '5', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '6', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '10', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '4', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '4', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '20', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '18', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '10', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '3', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '17', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '3', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '10', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '16', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '17', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '7', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '23', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '20', 'layer_norm', 'bias'), ('encoder', 'layers', '7', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '20', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '8', 'final_layer_norm', 'scale'), ('encoder', 'layers', '4', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '0', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '9', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '15', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '19', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'scale'), ('encoder', 'layers', '16', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '17', 'final_layer_norm', 'bias'), ('encoder', 'layers', '15', 'final_layer_norm', 'scale'), ('encoder', 'layers', '1', 'layer_norm', 'bias'), ('encoder', 'layers', '19', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '23', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '21', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '0', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '14', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '9', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '13', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '6', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '16', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '2', 'layer_norm', 'bias'), ('encoder', 'layers', '13', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '17', 'final_layer_norm', 'scale'), ('encoder', 'layers', '11', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '2', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '3', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'layer_norm', 'scale'), ('encoder', 'layers', '10', 'layer_norm', 'bias'), ('encoder', 'layers', '9', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '8', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '4', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '21', 'final_layer_norm', 'bias')}\n",
      "- This IS expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxWav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-lv60 and are newly initialized: {('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'layer_norm', 'scale'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'final_layer_norm', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'layer_norm', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'final_layer_norm', 'scale')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder checkpointing: False\n",
      "decoder scan: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at patrickvonplaten/bart-large-fp32 were not used when initializing FlaxBartForCausalLM: {('decoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '4', 'fc1', 'kernel'), ('decoder', 'layers', '2', 'encoder_attn_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '7', 'fc2', 'bias'), ('decoder', 'layers', '4', 'encoder_attn', 'out_proj', 'bias'), ('encoder', 'layers', '9', 'final_layer_norm', 'bias'), ('encoder', 'layers', '10', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '11', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '6', 'fc2', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '1', 'fc1', 'kernel'), ('decoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '3', 'final_layer_norm', 'bias'), ('decoder', 'layers', '6', 'encoder_attn', 'k_proj', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '2', 'fc2', 'bias'), ('encoder', 'layers', '11', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '5', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '8', 'fc2', 'bias'), ('encoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '10', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '4', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'fc1', 'kernel'), ('decoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '7', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '5', 'encoder_attn', 'k_proj', 'bias'), ('encoder', 'layers', '9', 'fc2', 'kernel'), ('decoder', 'layers', '7', 'fc2', 'bias'), ('encoder', 'layers', '6', 'fc1', 'bias'), ('decoder', 'layers', '4', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '9', 'encoder_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'fc1', 'kernel'), ('encoder', 'layers', '5', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '10', 'fc1', 'kernel'), ('decoder', 'layers', '6', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '10', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '5', 'encoder_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '4', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '3', 'fc2', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '9', 'fc1', 'bias'), ('decoder', 'layers', '10', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '7', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '3', 'final_layer_norm', 'bias'), ('decoder', 'layers', '6', 'fc1', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '2', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '11', 'fc1', 'kernel'), ('decoder', 'layers', '11', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '5', 'encoder_attn_layer_norm', 'kernel'), ('decoder', 'layers', '11', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '8', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '9', 'fc1', 'kernel'), ('decoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '7', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '8', 'fc1', 'kernel'), ('decoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '5', 'final_layer_norm', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '10', 'final_layer_norm', 'bias'), ('decoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '2', 'encoder_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '4', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '2', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '9', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '11', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'fc2', 'bias'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '3', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '3', 'fc1', 'bias'), ('encoder', 'layers', '8', 'fc1', 'bias'), ('encoder', 'layers', '10', 'fc2', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '9', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '8', 'encoder_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '7', 'fc1', 'bias'), ('decoder', 'layers', '6', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'kernel'), ('decoder', 'layers', '7', 'encoder_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '5', 'fc2', 'kernel'), ('decoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '10', 'fc2', 'kernel'), ('encoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '11', 'fc2', 'bias'), ('decoder', 'layers', '5', 'final_layer_norm', 'bias'), ('decoder', 'layers', '11', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '7', 'fc1', 'kernel'), ('decoder', 'layers', '4', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '2', 'fc2', 'bias'), ('decoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '2', 'fc1', 'bias'), ('encoder', 'embed_positions', 'kernel'), ('encoder', 'layers', '0', 'fc2', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '8', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '11', 'fc2', 'kernel'), ('decoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '6', 'encoder_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '7', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '6', 'encoder_attn_layer_norm', 'kernel'), ('decoder', 'layers', '4', 'encoder_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '3', 'fc1', 'kernel'), ('decoder', 'layers', '7', 'encoder_attn', 'k_proj', 'bias'), ('encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'layers', '4', 'fc2', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '2', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '2', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '4', 'encoder_attn_layer_norm', 'kernel'), ('decoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'fc1', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '5', 'fc1', 'bias'), ('decoder', 'layers', '7', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '9', 'fc2', 'bias'), ('encoder', 'layers', '10', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '2', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '0', 'fc2', 'bias'), ('encoder', 'layers', '2', 'fc1', 'kernel'), ('decoder', 'layers', '9', 'encoder_attn_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'kernel'), ('encoder', 'layers', '9', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '2', 'encoder_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '4', 'final_layer_norm', 'bias'), ('decoder', 'layers', '8', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '11', 'final_layer_norm', 'kernel'), ('shared', 'kernel'), ('decoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '5', 'encoder_attn', 'out_proj', 'bias'), ('encoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '3', 'encoder_attn', 'v_proj', 'bias'), ('decoder', 'layers', '8', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '10', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'fc2', 'kernel'), ('decoder', 'layers', '5', 'fc2', 'kernel'), ('decoder', 'layers', '9', 'encoder_attn', 'v_proj', 'bias'), ('decoder', 'layers', '4', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '8', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '3', 'encoder_attn_layer_norm', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '6', 'final_layer_norm', 'bias'), ('decoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '9', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '7', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '8', 'encoder_attn_layer_norm', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '3', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '10', 'encoder_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '3', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '4', 'fc2', 'kernel'), ('decoder', 'layers', '6', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layernorm_embedding', 'bias'), ('decoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '7', 'encoder_attn_layer_norm', 'kernel'), ('encoder', 'layers', '5', 'fc1', 'kernel'), ('decoder', 'layers', '11', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '8', 'final_layer_norm', 'bias'), ('decoder', 'layers', '3', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '8', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '4', 'fc1', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '6', 'fc2', 'kernel'), ('decoder', 'layers', '6', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '11', 'encoder_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '10', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '11', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '1', 'fc1', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '3', 'encoder_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '9', 'encoder_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '10', 'fc1', 'kernel'), ('encoder', 'layers', '11', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '8', 'fc2', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '5', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '4', 'fc1', 'bias'), ('encoder', 'layers', '0', 'fc1', 'bias'), ('decoder', 'layers', '11', 'fc1', 'kernel'), ('encoder', 'layers', '7', 'final_layer_norm', 'bias'), ('decoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '9', 'fc2', 'bias'), ('decoder', 'layers', '4', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '0', 'fc1', 'bias'), ('encoder', 'layers', '5', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '10', 'fc1', 'bias'), ('decoder', 'layers', '6', 'final_layer_norm', 'bias'), ('decoder', 'layers', '10', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '5', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '3', 'fc2', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '6', 'fc1', 'bias'), ('decoder', 'layers', '2', 'final_layer_norm', 'bias'), ('decoder', 'layers', '5', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '11', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '5', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '3', 'fc2', 'kernel'), ('decoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '8', 'final_layer_norm', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '9', 'fc1', 'bias'), ('encoder', 'layers', '7', 'fc2', 'kernel'), ('decoder', 'layers', '4', 'encoder_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '9', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '7', 'final_layer_norm', 'bias'), ('decoder', 'layers', '8', 'fc1', 'bias'), ('decoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '6', 'fc2', 'kernel'), ('encoder', 'layers', '3', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '2', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'bias'), ('decoder', 'layers', '6', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '2', 'fc2', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('encoder', 'embed_tokens', 'kernel'), ('decoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '3', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '10', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '8', 'fc2', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '9', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '8', 'encoder_attn', 'out_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '3', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '5', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '7', 'fc2', 'kernel'), ('encoder', 'layers', '6', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '7', 'encoder_attn', 'out_proj', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '9', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '5', 'fc2', 'bias'), ('decoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '10', 'fc2', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '10', 'encoder_attn', 'out_proj', 'bias'), ('encoder', 'layers', '4', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '9', 'fc1', 'kernel'), ('decoder', 'layers', '10', 'encoder_attn_layer_norm', 'kernel'), ('decoder', 'layers', '7', 'fc1', 'bias'), ('decoder', 'layers', '7', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '3', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '11', 'fc2', 'bias'), ('decoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'layers', '11', 'encoder_attn_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '6', 'encoder_attn', 'out_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '5', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '10', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '6', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '4', 'encoder_attn', 'k_proj', 'bias'), ('encoder', 'layers', '3', 'fc1', 'bias'), ('decoder', 'layers', '4', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '2', 'encoder_attn', 'v_proj', 'bias'), ('decoder', 'layers', '2', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'final_layer_norm', 'bias'), ('decoder', 'layers', '4', 'encoder_attn_layer_norm', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '6', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '2', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '7', 'encoder_attn', 'q_proj', 'bias'), ('decoder', 'layers', '9', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '11', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '10', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'fc2', 'kernel'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'fc1', 'bias'), ('decoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '3', 'fc1', 'kernel'), ('encoder', 'layers', '8', 'fc1', 'kernel'), ('decoder', 'layers', '9', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'bias'), ('encoder', 'layers', '10', 'fc2', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '9', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '2', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '7', 'fc1', 'kernel'), ('decoder', 'layers', '6', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '8', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '11', 'final_layer_norm', 'bias'), ('decoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '11', 'fc2', 'kernel'), ('decoder', 'layers', '5', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '11', 'encoder_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '10', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '1', 'fc2', 'bias'), ('decoder', 'layers', '5', 'fc2', 'bias'), ('decoder', 'layers', '4', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '4', 'final_layer_norm', 'bias'), ('encoder', 'layers', '2', 'fc2', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '2', 'fc1', 'kernel'), ('decoder', 'layers', '8', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '0', 'fc2', 'kernel'), ('decoder', 'layers', '8', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '9', 'final_layer_norm', 'bias'), ('decoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '8', 'encoder_attn_layer_norm', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '10', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '4', 'fc2', 'bias'), ('decoder', 'layers', '6', 'encoder_attn', 'v_proj', 'bias'), ('decoder', 'layers', '7', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '7', 'encoder_attn_layer_norm', 'bias'), ('encoder', 'layers', '5', 'fc1', 'bias'), ('decoder', 'layers', '3', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '8', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '7', 'encoder_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '11', 'fc1', 'bias'), ('decoder', 'layers', '4', 'fc2', 'kernel'), ('encoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '6', 'fc2', 'bias'), ('decoder', 'layers', '6', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'fc1', 'kernel'), ('decoder', 'layers', '5', 'fc1', 'kernel'), ('decoder', 'layers', '11', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '9', 'fc2', 'kernel'), ('decoder', 'layers', '2', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'fc2', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '3', 'encoder_attn', 'out_proj', 'bias'), ('encoder', 'layers', '4', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '10', 'encoder_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '9', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '10', 'fc1', 'bias'), ('decoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '5', 'encoder_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '8', 'fc2', 'bias'), ('decoder', 'layers', '3', 'encoder_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '8', 'encoder_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '9', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '11', 'fc1', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '3', 'encoder_attn_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '6', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '5', 'self_attn_layer_norm', 'bias'), ('encoder', 'layernorm_embedding', 'kernel'), ('decoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '11', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '8', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '3', 'fc2', 'bias')}\n",
      "- This IS expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBartForCausalLM were not initialized from the model checkpoint at patrickvonplaten/bart-large-fp32 and are newly initialized: {('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'fc2', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'fc2', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'fc1', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'fc1', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'v_proj', 'bias')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder checkpointing: False\n",
      "encoder scan: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 12:56:22.149184: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder checkpointing: False\n",
      "decoder scan: True\n",
      "encoder checkpointing: False\n",
      "encoder scan: True\n",
      "decoder checkpointing: False\n",
      "decoder scan: True\n",
      "decoder checkpointing: False\n",
      "decoder scan: True\n"
     ]
    }
   ],
   "source": [
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "fx_model = CustomFlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_from_pt=tiny, decoder_from_pt=True)\n",
    "\n",
    "fx_model.config.decoder_start_token_id = fx_model.config.decoder.bos_token_id\n",
    "fx_model.config.pad_token_id = fx_model.config.decoder.pad_token_id\n",
    "fx_model.config.eos_token_id = fx_model.config.decoder.eos_token_id\n",
    "fx_model.config.processor_class = \"Wav2Vec2Processor\"\n",
    "\n",
    "# check if generation works\n",
    "fx_out = fx_model.generate(jnp.ones((1, 2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89065aee-d19a-4ce4-9a48-62e4903393aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-lv60 were not used when initializing Wav2Vec2Model: ['project_hid.weight', 'project_q.weight', 'project_q.bias', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at patrickvonplaten/bart-large-fp32 were not used when initializing BartForCausalLM: ['encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.6.fc1.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.7.fc1.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.5.fc1.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.7.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.8.fc1.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.7.fc2.weight', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.fc2.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'shared.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.11.fc1.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.6.fc2.bias', 'encoder.layers.10.fc1.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layernorm_embedding.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.4.fc2.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.8.fc2.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.2.fc2.weight', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.0.fc1.weight', 'encoder.layers.7.fc1.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.11.fc2.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.fc2.bias', 'encoder.embed_tokens.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.0.fc2.bias', 'encoder.layernorm_embedding.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.6.fc1.bias', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.6.fc2.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.embed_positions.weight', 'encoder.layers.10.fc2.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.9.fc2.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.8.fc2.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.1.fc1.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.10.fc2.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.3.fc2.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.2.fc1.bias', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.11.fc1.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.5.fc2.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.8.fc1.bias', 'encoder.layers.10.fc1.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.9.self_attn_layer_norm.weight', 'encoder.layers.9.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.4.fc2.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.9.fc1.bias', 'encoder.layers.11.fc2.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.9.fc1.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc2.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at patrickvonplaten/bart-large-fp32 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pt_model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)\n",
    "pt_model.config.decoder_start_token_id = pt_model.config.decoder.bos_token_id\n",
    "pt_model.config.pad_token_id = pt_model.config.decoder.pad_token_id\n",
    "pt_model.config.eos_token_id = pt_model.config.decoder.eos_token_id\n",
    "pt_model.config.processor_class = \"Wav2Vec2Processor\"\n",
    "\n",
    "# check if generation works\n",
    "pt_out = pt_model.generate(torch.ones((1, 2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4175594d-e4a4-482b-ab24-62846e53183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)\n",
    "processor = AutoProcessor.from_pretrained(encoder_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d28176-0f92-45be-a02b-47874615f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fx_model.config.decoder_start_token_id or pt_model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d375fc30-b84d-488e-bcbd-4d95acc66ac6",
   "metadata": {},
   "source": [
    "### 6. Convert unrolled weights to scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9759ecda-2e85-4f43-92a2-b27c113d791c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /tmp/tmpw4933bqz were not used when initializing FlaxSpeechEncoderDecoderModel: {('decoder', 'lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Convert the PT model to FX to enable manipulation of param dicts (PT state dict -> FX param dict)\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    pt_model.save_pretrained(tmpdirname)\n",
    "    pt_model_to_fx = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(tmpdirname, from_pt=True)\n",
    "    \n",
    "def unrolled_to_scanned(model):\n",
    "    params = model.params\n",
    "    new_enc_params = {}\n",
    "    # get the key of a scanned module\n",
    "    for k in flatten_dict(params['encoder']['encoder']['layers']['0']):\n",
    "        # stack the weights for each layer of the scanned module into one matrix\n",
    "        new_enc_params[k] = jnp.stack([flatten_dict(params['encoder']['encoder']['layers'][str(i)])[k] for i in range(model.config.encoder.num_hidden_layers)])\n",
    "    # append the correct prefix to the scanned modules' keys\n",
    "    new_enc_params = unflatten_dict({('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers'): unflatten_dict(new_enc_params)})\n",
    "    \n",
    "    # repeat for the decoder (note that the key 'layers' appears one index to the right than in the encoder, thus we'll treat the encoder and decoder independently for now)\n",
    "    new_dec_params = {}\n",
    "    for k in flatten_dict(params['decoder']['model']['decoder']['layers']['0']):\n",
    "        new_dec_params[k] = jnp.stack([flatten_dict(params['decoder']['model']['decoder']['layers'][str(i)])[k] for i in range(model.config.decoder.decoder_layers)])\n",
    "    new_dec_params = unflatten_dict({('model', 'decoder', 'layers', 'FlaxBartDecoderLayers'): unflatten_dict(new_dec_params)})\n",
    "    \n",
    "    # combine the encoder and decoder parameters\n",
    "    new_params = {'encoder': new_enc_params, 'decoder': new_dec_params}\n",
    "    new_params = flatten_dict(new_params)\n",
    "    \n",
    "    # append parameters for non-scanned modules (i.e. all modules that do not contain the key 'layers')\n",
    "    for k in flatten_dict(params):\n",
    "        if 'layers' not in k:\n",
    "            new_params[k] = flatten_dict(params)[k]\n",
    "\n",
    "    return unflatten_dict(new_params)\n",
    "\n",
    "fx_model.params = unrolled_to_scanned(pt_model_to_fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc64d65-d413-4bd5-a43e-b2cec17f4487",
   "metadata": {},
   "source": [
    "### 7. Resample speech dataset if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c090ec66-7894-4f16-ae02-547f29bd55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Torch audio in this resampling step for convinience\n",
    "dataset_sampling_rate = next(iter(raw_datasets.values())).features[audio_column_name].sampling_rate\n",
    "if dataset_sampling_rate != feature_extractor.sampling_rate:\n",
    "    raw_datasets = raw_datasets.cast_column(\n",
    "        audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20ebb8-bda1-433a-9150-2e7efbbc2aaa",
   "metadata": {},
   "source": [
    "### 8. Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6edcbe-ec3c-4416-b10b-11921d3f7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "max_input_length = int(max_duration_in_seconds * feature_extractor.sampling_rate)\n",
    "min_input_length = int(min_duration_in_seconds * feature_extractor.sampling_rate)\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "158d9931-1164-4df1-8f82-f89b7c2889c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate data to max_samples\n",
    "if max_train_samples is not None:\n",
    "        raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(max_train_samples))\n",
    "\n",
    "if max_eval_samples is not None:\n",
    "    raw_datasets[\"eval\"] = raw_datasets[\"eval\"].select(range(max_eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9268b414-4593-48ca-a006-dc8be2a61e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # process audio\n",
    "    sample = batch[audio_column_name]\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    # process audio length\n",
    "    batch[model_input_name] = inputs.input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    # process targets\n",
    "    input_str = batch[text_column_name].lower() if do_lower_case else batch[text_column_name]\n",
    "    batch[\"labels\"] = tokenizer(input_str).input_ids\n",
    "    batch[\"labels_length\"] = len(batch[\"labels\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fad9276-c923-4b41-9b2f-a23bb6fc38d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess train dataset: 100%|| 1427/1427 [00:16<00:00, 84.58ex/s]\n",
      "preprocess train dataset: 100%|| 135/135 [00:00<00:00, 193.54ex/s]\n"
     ]
    }
   ],
   "source": [
    "vectorized_datasets = raw_datasets.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=next(iter(raw_datasets.values())).column_names,\n",
    "            num_proc=preprocessing_num_workers,\n",
    "            desc=\"preprocess train dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b025286e-c214-4983-bbef-6ee92e4e4b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:00<00:00, 30.71ba/s]\n",
      "100%|| 1/1 [00:00<00:00, 733.78ba/s]\n"
     ]
    }
   ],
   "source": [
    "# filter data with inputs shorter than min_input_length or longer than max_input_length\n",
    "def is_audio_in_length_range(length):\n",
    "    return length > min_input_length and length < max_input_length\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_audio_in_length_range,\n",
    "    num_proc=num_workers,\n",
    "    input_columns=[\"input_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b963df1-e2d2-4402-91ae-60142e39d3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00, 299.72ba/s]\n",
      "100%|| 1/1 [00:00<00:00, 563.83ba/s]\n"
     ]
    }
   ],
   "source": [
    "# filter data with targets shorter than min_target_length or longer than max_target_length\n",
    "def is_labels_in_length_range(length):\n",
    "    return length > min_target_length and length < max_target_length\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_labels_in_length_range,\n",
    "    num_proc=num_workers,\n",
    "    input_columns=[\"labels_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f96e4-4999-4279-9e84-c3890f60d80f",
   "metadata": {},
   "source": [
    "### 9. Define DataCollators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "779d187a-4309-45a1-a1e1-7b672b38c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch DataCollator\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b3f1949-a60c-4c08-9ff3-c7bc620302fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax DataCollator\n",
    "@flax.struct.dataclass\n",
    "class FlaxDataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    input_padding: Union[bool, str] = \"longest\"\n",
    "    target_padding: Union[bool, str] = \"max_length\"\n",
    "    max_input_length: Optional[float] = None\n",
    "    max_target_length: Optional[int] = None\n",
    "    pad_input_to_multiple_of: Optional[int] = None\n",
    "    pad_target_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # reformat list to dict and set to pytorch format\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=self.input_padding,\n",
    "            pad_to_multiple_of=self.pad_input_to_multiple_of,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=self.target_padding,\n",
    "            pad_to_multiple_of=self.pad_target_to_multiple_of,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().item():\n",
    "            labels = labels[:, 1:]\n",
    "            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]\n",
    "\n",
    "        decoder_input_ids = shift_tokens_right(labels, self.decoder_start_token_id)\n",
    "\n",
    "        # replace padding with -100 to ignore correctly when computing the loss\n",
    "        labels = np.ma.array(labels, mask=np.not_equal(labels_batch.attention_mask, 1))\n",
    "        labels = labels.filled(fill_value=-100)\n",
    "\n",
    "        batch[\"inputs\"] = batch.pop(\"input_values\")\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "        return batch\n",
    "    \n",
    "def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shift label ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_label_ids = np.zeros_like(label_ids)\n",
    "    shifted_label_ids[:, 1:] = label_ids[:, :-1]\n",
    "    shifted_label_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    return shifted_label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69218427-b83a-4499-b910-73f68d34157f",
   "metadata": {},
   "source": [
    "### 10. Define length grouped sampler (PT and FX compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b356f74f-4033-466d-a6c3-47462387e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_indices(\n",
    "    dataset, batch_size: int, rng: Optional[List[int]] = None, mega_batch_mult: Optional[int] = None\n",
    ") -> np.array:\n",
    "    lengths = dataset[\"input_length\"]\n",
    "\n",
    "    # Default for mega_batch_mult: 50 or the number to get 4 megabatches, whichever is smaller.\n",
    "    if mega_batch_mult is None:\n",
    "        mega_batch_mult = min(len(lengths) // (batch_size * 4), 50)\n",
    "        # Just in case, for tiny datasets\n",
    "        if mega_batch_mult == 0:\n",
    "            mega_batch_mult = 1\n",
    "\n",
    "    # We need to use JAX for the random permutation as the PRNG key will be set based on the seed outside of the sampler.\n",
    "    num_samples = len(lengths)\n",
    "    indices = jax.random.permutation(rng, np.arange(num_samples)) if rng is not None else np.arange(num_samples)\n",
    "\n",
    "    megabatch_size = mega_batch_mult * batch_size\n",
    "    megabatches = [indices[i : i + megabatch_size].tolist() for i in range(0, len(lengths), megabatch_size)]\n",
    "    megabatches = [list(sorted(megabatch, key=lambda i: lengths[i], reverse=True)) for megabatch in megabatches]\n",
    "\n",
    "    # The rest is to get the biggest batch first.\n",
    "    # Since each megabatch is sorted by descending length, the longest element is the first\n",
    "    megabatch_maximums = [lengths[megabatch[0]] for megabatch in megabatches]\n",
    "    max_idx = np.argmax(megabatch_maximums).item()\n",
    "    # Switch to put the longest batch in first position\n",
    "    # (note that this is different to the PT grouped sampler in which we only put the longest element in the first position, and not its batch)\n",
    "    megabatches[0], megabatches[max_idx] = megabatches[max_idx], megabatches[0]\n",
    "\n",
    "    megabatches = np.array([i for megabatch in megabatches for i in megabatch])\n",
    "\n",
    "    return megabatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6facfb32-f831-4b2a-9210-2fe9df1a35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group samples into batch splits\n",
    "def generate_batch_splits(samples_idx: jnp.ndarray, batch_size: int) -> jnp.ndarray:\n",
    "    num_samples = len(samples_idx)\n",
    "    samples_to_remove = num_samples % batch_size\n",
    "\n",
    "    if samples_to_remove != 0:\n",
    "        samples_idx = samples_idx[:-samples_to_remove]\n",
    "    sections_split = num_samples // batch_size\n",
    "    batch_idx = np.split(samples_idx, sections_split)\n",
    "    return batch_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8416e98-3a84-4958-a03a-ed4167266fef",
   "metadata": {},
   "source": [
    "### 11. Helper funcitons for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2915d6a0-536c-4c41-987c-28bcfe024114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_almost_equals(a: np.ndarray, b: np.ndarray, tol: float = 1e-2):\n",
    "    \"\"\"Assert whether the maximum absolute difference between two NumPy arrays a and b is within a given tolerance tol. \n",
    "    Due to the pad_to_multiple_of nature of the FlaxDataCollator, the length of the Flax array a will always be greater than \n",
    "    or equal to the length of the PyTorch array b. If a and b are of different lengths, array a (Flax, padded) will be \n",
    "    reshaped to the shape of b (PyTorch).\"\"\"\n",
    "    if a.shape != b.shape:\n",
    "        a = a[:, :b.shape[1]]\n",
    "    \n",
    "    diff = np.abs((a - b)).max()\n",
    "    if diff < tol:\n",
    "        print(f\" Difference between Flax and PyTorch is {diff} (< {tol})\")\n",
    "    else:\n",
    "        print(f\" Difference between Flax and PyTorch is {diff} (>= {tol}),\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e8c23b8-4637-4da9-9f1a-49ce7cf8aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_dict_equal(a: dict, b: dict, tol: float = 1e-2):\n",
    "    if a.keys() != b.keys():\n",
    "        print(\" Dictionary keys for PyTorch and Flax do not match\")\n",
    "    results_fail = []\n",
    "    results_correct = []\n",
    "\n",
    "    results_fail_rel = []\n",
    "    results_correct_rel = []\n",
    "    for k in a:\n",
    "        ak_norm = np.linalg.norm(a[k])\n",
    "        bk_norm = np.linalg.norm(b[k])\n",
    "        diff = np.abs(ak_norm - bk_norm)\n",
    "        diff_rel = np.abs(ak_norm - bk_norm) / np.abs(ak_norm)\n",
    "        if diff < tol:\n",
    "            results_correct.append(f\" Layer {k} diff is {diff} < {tol}).\")\n",
    "        else:\n",
    "            results_fail.append(f\" Layer {k} has PT grad norm {bk_norm} and flax grad norm {ak_norm}.\")\n",
    "        if diff_rel < tol:\n",
    "            results_correct_rel.append(f\" Layer {k} rel diff is {diff} < {tol}).\")\n",
    "        else:\n",
    "            results_fail_rel.append(f\" Layer {k} has PT grad norm {bk_norm} and flax grad norm {ak_norm}.\")\n",
    "    return results_fail_rel, results_correct_rel, results_fail, results_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2357b941-5d1a-423d-84e2-9f95e4ab4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def kl_divergence(a: np.ndarray, b:np.ndarray, epsilon=1e-6, tol: float = 1e-2):\n",
    "    \"\"\"Epsilon is used here to avoid conditional code for checking that neither p(a) nor p(b) is equal to 0.\"\"\"\n",
    "    if a.shape[1] != b.shape[1]:\n",
    "        a = a[:, :b.shape[1], :]\n",
    "        \n",
    "    p_a = softmax(a) + epsilon\n",
    "    p_b = softmax(b) + epsilon\n",
    "    divergence = np.sum(p_b * np.log(p_b / p_a))\n",
    "    if divergence < tol:\n",
    "        print(f\" KL divergence between Flax and PyTorch is {divergence} (< {tol})\")\n",
    "    else:\n",
    "        print(f\" KL divergence between Flax and PyTorch is {divergence} (>= {tol})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faecec1d-884c-4baa-b636-27f9a7556df4",
   "metadata": {},
   "source": [
    "### 12. Instantiate data collators and generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47ab184f-a169-4a8b-99ff-64a7829e648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PT and FX DataCollators\n",
    "pt_data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=pt_model.config.decoder_start_token_id,\n",
    "    )\n",
    "\n",
    "fx_data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=fx_model.config.decoder_start_token_id,\n",
    "        input_padding=\"longest\",\n",
    "        target_padding=\"max_length\",\n",
    "        max_target_length=max_target_length,\n",
    "        pad_input_to_multiple_of=pad_input_to_multiple_of,\n",
    "        pad_target_to_multiple_of=pad_target_to_multiple_of,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6d2d15d-6381-45a7-855d-0da4656c4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAX seed and generate PRNG for stochasic operations\n",
    "seed = 0\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "rng, input_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8fe6bf3-6a77-4432-a1c6-02a0c5167636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll naively create our batches through random shuffling and no grouping by length\n",
    "num_train_samples = len(vectorized_datasets[\"train\"])\n",
    "train_samples_idx = jax.random.permutation(input_rng, np.arange(num_train_samples))\n",
    "train_batch_idx = generate_batch_splits(train_samples_idx, batch_size_per_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b174426-c729-48ce-ab8f-3161dc5d6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt: we'll use the grouped sampler\n",
    "train_samples_idx = get_grouped_indices(vectorized_datasets[\"train\"], batch_size_per_update, input_rng)\n",
    "train_batch_idx = generate_batch_splits(train_samples_idx, batch_size_per_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a8734b5-dd02-4860-bc57-b6749752e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the first training batch\n",
    "batch_idx = train_batch_idx[0]\n",
    "samples = [vectorized_datasets[\"train\"][int(idx)] for idx in batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d81d159-1d3c-4bb5-b5be-17d63e13a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_batch = fx_data_collator(samples)\n",
    "pt_batch = pt_data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c61f7e33-3503-426e-a761-f42cb153b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Flax inputs to PyTorch (optional)\n",
    "#pt_batch = {k: torch.tensor(v.tolist()) for k, v in fx_batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a13e0e-b9ff-4598-8981-b68898a308c7",
   "metadata": {},
   "source": [
    "### 13. Check that the inputs are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d2993ad-e8fa-4bad-8039-0787b46bfd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['attention_mask', 'inputs', 'labels', 'decoder_input_ids']),\n",
       " dict_keys(['input_values', 'attention_mask', 'labels']))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_fx_keys = [\"inputs\", \"labels\", \"decoder_input_ids\"]\n",
    "expected_pt_keys = [\"input_values\", \"labels\"]\n",
    "\n",
    "for expected_fx_key in expected_fx_keys:\n",
    "    assert expected_fx_key in fx_batch, f\"{expected_fx_key} not in Flax batched inputs\"\n",
    "\n",
    "for expected_pt_key in expected_pt_keys:\n",
    "    assert expected_pt_key in pt_batch, f\"{expected_pt_key} not in PyTorch batched inputs\"    \n",
    "\n",
    "# Expect the keys between Flax and PyTorch to be different, this is just for observation\n",
    "fx_batch.keys(), pt_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61dc7ba5-6608-47cf-a9c6-197f9852101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Difference between Flax and PyTorch is 0.0 (< 0.01)\n",
      " Difference between Flax and PyTorch is 0 (< 0.01)\n",
      " Difference between Flax and PyTorch is 0 (< 0.01)\n"
     ]
    }
   ],
   "source": [
    "assert_almost_equals(fx_batch['inputs'], pt_batch['input_values'].numpy())\n",
    "assert_almost_equals(fx_batch['labels'], pt_batch['labels'].numpy())\n",
    "if 'attention_mask' in fx_batch.keys() and pt_batch.keys():\n",
    "    assert_almost_equals(fx_batch['attention_mask'], pt_batch['attention_mask'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d5329-9e06-4d03-af9b-ae853e174ed1",
   "metadata": {},
   "source": [
    "### 14. Run a training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6921b1ff-1e77-4047-8482-ab49f76936cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch)\n",
    "pt_logits = pt_outputs.logits\n",
    "pt_loss = pt_outputs.loss\n",
    "pt_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c2760c3-4d97-4fe9-ac93-e83d55ff9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax cross entropy loss\n",
    "def loss_fn(logits, labels):\n",
    "    vocab_size = logits.shape[-1]\n",
    "    loss = optax.softmax_cross_entropy(logits, onehot(labels, vocab_size))\n",
    "    # ignore padded tokens from loss, i.e. where labels are not set to -100\n",
    "    padding = labels >= 0\n",
    "    loss = loss * padding\n",
    "    loss = loss.sum() / padding.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53212c98-0e5f-402b-840b-495819496339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax training step (single device)\n",
    "def fx_train_step(fx_model, fx_batch):\n",
    "    def compute_loss(params):\n",
    "        labels = fx_batch.pop(\"labels\")\n",
    "        outputs = fx_model(**fx_batch, params=params)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        return loss, outputs\n",
    "\n",
    "    grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
    "    (loss, outputs), grad = grad_fn(fx_model.params)\n",
    "    \n",
    "    return loss, outputs, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f54a60cb-619c-449c-bb88-f9b5056e57b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder checkpointing: False\n",
      "encoder scan: True\n",
      "decoder checkpointing: False\n",
      "decoder scan: True\n"
     ]
    }
   ],
   "source": [
    "fx_loss, fx_outputs, fx_grad = fx_train_step(fx_model, fx_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f993ab-2698-4af5-9c5a-b1d9d4ca2f68",
   "metadata": {},
   "source": [
    "### 15. Compare outputs for the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "771bb04e-a1e0-4276-8330-ac7f129cbe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking encoder last hidden states match--------------------------\n",
      " Difference between Flax and PyTorch is 0.00025206804275512695 (< 0.01)\n",
      "--------------------------Checking logits match--------------------------\n",
      "Flax logits shape: (2, 31, 50265), PyTorch logits shape: torch.Size([2, 17, 50265])\n",
      " Difference between Flax and PyTorch is 0.00012969970703125 (< 0.01)\n",
      " KL divergence between Flax and PyTorch is 0.0002297908067703247 (< 0.01)\n",
      "--------------------------Checking losses match--------------------------\n",
      "Flax loss: 24.808961868286133, PyTorch loss: 24.80891227722168\n",
      " Difference between Flax and PyTorch is 4.9591064453125e-05 (< 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking encoder last hidden states match--------------------------\")\n",
    "assert_almost_equals(fx_outputs.encoder_last_hidden_state, pt_outputs.encoder_last_hidden_state.detach().numpy())\n",
    "    \n",
    "print(\"--------------------------Checking logits match--------------------------\")\n",
    "print(f\"Flax logits shape: {fx_outputs.logits.shape}, PyTorch logits shape: {pt_logits.shape}\")\n",
    "assert_almost_equals(fx_outputs.logits, pt_logits.detach().numpy())\n",
    "kl_divergence(fx_outputs.logits, pt_logits.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking losses match--------------------------\")\n",
    "print(f\"Flax loss: {fx_loss}, PyTorch loss: {pt_loss}\")\n",
    "assert_almost_equals(fx_loss, pt_loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d0f8c-c34d-4931-9d4c-778f9a67b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking encoder last hidden states match--------------------------\n",
      " Difference between Flax and PyTorch is 0.0006010532379150391 (< 0.01)\n",
      "--------------------------Checking logits match--------------------------\n",
      "Flax logits shape: (2, 31, 50265), PyTorch logits shape: torch.Size([2, 21, 50265])\n",
      " Difference between Flax and PyTorch is 4.9591064453125e-05 (< 0.01)\n",
      " KL divergence between Flax and PyTorch is 0.000494436826556921 (< 0.01)\n",
      "--------------------------Checking losses match--------------------------\n",
      "Flax loss: 25.75644302368164, PyTorch loss: 25.756454467773438\n",
      " Difference between Flax and PyTorch is 1.1444091796875e-05 (< 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking encoder last hidden states match--------------------------\")\n",
    "assert_almost_equals(fx_outputs.encoder_last_hidden_state, pt_outputs.encoder_last_hidden_state.detach().numpy())\n",
    "    \n",
    "print(\"--------------------------Checking logits match--------------------------\")\n",
    "print(f\"Flax logits shape: {fx_outputs.logits.shape}, PyTorch logits shape: {pt_logits.shape}\")\n",
    "assert_almost_equals(fx_outputs.logits, pt_logits.detach().numpy())\n",
    "kl_divergence(fx_outputs.logits, pt_logits.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking losses match--------------------------\")\n",
    "print(f\"Flax loss: {fx_loss}, PyTorch loss: {pt_loss}\")\n",
    "assert_almost_equals(fx_loss, pt_loss.detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
