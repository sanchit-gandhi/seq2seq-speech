{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0b3b1b-ec40-40fd-b8b9-0efff536fb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanchitgandhi/venv/lib/python3.8/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxSpeechEncoderDecoderModel\n",
    "from models.modeling_flax_speech_encoder_decoder import FlaxSpeechEncoderDecoderModel as CustomFlaxSpeechEncoderDecoderModel\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "import numpy as np\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1549a8b-d889-4fce-883e-66821fb084a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_id = 'hf-internal-testing/tiny-random-wav2vec2'\n",
    "decoder_id = 'hf-internal-testing/tiny-random-bart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41d0e65-0218-497e-92a7-70aeb12abc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 were not used when initializing FlaxWav2Vec2Model: {('project_q', 'bias'), ('project_q', 'kernel'), ('project_hid', 'bias'), ('project_hid', 'kernel'), ('quantizer', 'codevectors'), ('quantizer', 'weight_proj', 'bias'), ('quantizer', 'weight_proj', 'kernel'), ('lm_head', 'bias'), ('lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxWav2Vec2Model were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: {('feature_extractor', 'conv_layers', '1', 'layer_norm', 'bias'), ('feature_extractor', 'conv_layers', '1', 'layer_norm', 'scale'), ('feature_extractor', 'conv_layers', '2', 'layer_norm', 'bias'), ('feature_extractor', 'conv_layers', '2', 'layer_norm', 'scale')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-bart were not used when initializing FlaxBartForCausalLM: {('qa_outputs', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('final_logits_bias',), ('model', 'encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'fc1', 'bias'), ('model', 'encoder', 'layers', '1', 'fc1', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'fc2', 'bias'), ('encoder', 'layernorm_embedding', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('classification_head', 'dense', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '1', 'fc1', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'embed_positions', 'embedding'), ('model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'fc2', 'kernel'), ('decoder', 'layernorm_embedding', 'bias'), ('decoder', 'layernorm_embedding', 'scale'), ('model', 'shared', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layernorm_embedding', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('shared', 'kernel'), ('encoder', 'embed_tokens', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc2', 'kernel'), ('model', 'encoder', 'layers', '1', 'fc2', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'fc1', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'scale'), ('classification_head', 'out_proj', 'kernel'), ('decoder', 'layers', '0', 'fc1', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '1', 'fc2', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'embed_tokens', 'embedding'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'bias'), ('qa_outputs', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('model', 'encoder', 'embed_tokens', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc1', 'kernel'), ('model', 'encoder', 'layers', '1', 'fc1', 'kernel'), ('encoder', 'embed_positions', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'scale'), ('model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'fc2', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'fc1', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'fc1', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layernorm_embedding', 'bias'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'scale'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '0', 'fc2', 'kernel'), ('classification_head', 'dense', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'fc2', 'bias'), ('decoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '0', 'final_layer_norm', 'scale'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'scale'), ('decoder', 'layers', '1', 'final_layer_norm', 'scale'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'fc2', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'bias'), ('decoder', 'layers', '0', 'fc1', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layernorm_embedding', 'bias'), ('decoder', 'layers', '1', 'fc2', 'kernel'), ('lm_head', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc2', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '0', 'fc1', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('classification_head', 'out_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'fc2', 'bias'), ('encoder', 'layers', '1', 'fc1', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'bias'), ('model', 'encoder', 'embed_positions', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "hf_model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_from_pt=True, decoder_from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0efa3c4-8732-40c7-9006-8e6e423b5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hf_model.config\n",
    "num_decoder_layers = config.decoder.decoder_layers\n",
    "num_encoder_layers = config.encoder.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d57f5ea3-91ae-450d-8d79-fd0415744354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder checkpointing: False\n",
      "encoder scan: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 were not used when initializing FlaxWav2Vec2Model: {('encoder', 'layers', '0', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '2', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '2', 'layer_norm', 'kernel'), ('encoder', 'layers', '0', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '3', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '1', 'attention', 'k_proj', 'kernel'), ('project_hid', 'bias'), ('encoder', 'layers', '3', 'layer_norm', 'kernel'), ('encoder', 'layers', '2', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '3', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '2', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '3', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', '0', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'attention', 'out_proj', 'kernel'), ('quantizer', 'weight_proj', 'kernel'), ('encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('encoder', 'layers', '1', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '0', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '2', 'layer_norm', 'bias'), ('encoder', 'layers', '0', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '2', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '1', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '3', 'layer_norm', 'bias'), ('encoder', 'layers', '0', 'attention', 'k_proj', 'kernel'), ('project_q', 'kernel'), ('encoder', 'layers', '0', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '3', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '3', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '2', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '0', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', '3', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', '3', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('quantizer', 'weight_proj', 'bias'), ('encoder', 'layers', '3', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'attention', 'v_proj', 'bias'), ('lm_head', 'kernel'), ('encoder', 'layers', '2', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '2', 'final_layer_norm', 'kernel'), ('project_q', 'bias'), ('encoder', 'layers', '0', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '3', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '0', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', '3', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '0', 'layer_norm', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('encoder', 'layers', '3', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', '1', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '3', 'final_layer_norm', 'kernel'), ('lm_head', 'bias'), ('encoder', 'layers', '0', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '2', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'layer_norm', 'kernel'), ('encoder', 'layers', '2', 'final_layer_norm', 'bias'), ('encoder', 'layers', '2', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '3', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'feed_forward', 'intermediate_dense', 'kernel'), ('encoder', 'layers', '2', 'attention', 'out_proj', 'kernel'), ('project_hid', 'kernel'), ('encoder', 'layers', '0', 'layer_norm', 'bias'), ('encoder', 'layers', '2', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'attention', 'q_proj', 'bias'), ('quantizer', 'codevectors'), ('encoder', 'layers', '3', 'final_layer_norm', 'bias'), ('encoder', 'layers', '2', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', '1', 'layer_norm', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', '1', 'feed_forward', 'output_dense', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxWav2Vec2Model were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: {('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'feed_forward', 'output_dense', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'feed_forward', 'intermediate_dense', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'k_proj', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'out_proj', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'feed_forward', 'output_dense', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'layer_norm', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'v_proj', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'k_proj', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'layer_norm', 'scale'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'out_proj', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'v_proj', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'q_proj', 'kernel'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'final_layer_norm', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'attention', 'q_proj', 'bias'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'final_layer_norm', 'scale'), ('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers', 'feed_forward', 'intermediate_dense', 'kernel'), ('feature_extractor', 'conv_layers', '1', 'layer_norm', 'bias'), ('feature_extractor', 'conv_layers', '1', 'layer_norm', 'scale'), ('feature_extractor', 'conv_layers', '2', 'layer_norm', 'bias'), ('feature_extractor', 'conv_layers', '2', 'layer_norm', 'scale')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder checkpointing: False\n",
      "decoder scan: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-bart were not used when initializing FlaxBartForCausalLM: {('qa_outputs', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('final_logits_bias',), ('model', 'encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '0', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'fc1', 'bias'), ('model', 'decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'fc1', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'fc2', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layernorm_embedding', 'kernel'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('model', 'decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'fc1', 'bias'), ('classification_head', 'dense', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'fc2', 'kernel'), ('model', 'encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '1', 'fc1', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'embed_positions', 'embedding'), ('model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'fc2', 'kernel'), ('decoder', 'layernorm_embedding', 'bias'), ('decoder', 'layernorm_embedding', 'scale'), ('model', 'shared', 'kernel'), ('decoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('model', 'decoder', 'layers', '1', 'encoder_attn_layer_norm', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layernorm_embedding', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('shared', 'kernel'), ('encoder', 'embed_tokens', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc2', 'kernel'), ('model', 'encoder', 'layers', '1', 'fc2', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'fc1', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'fc1', 'bias'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '0', 'fc2', 'kernel'), ('classification_head', 'out_proj', 'kernel'), ('decoder', 'layers', '0', 'fc1', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '1', 'fc2', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'embed_tokens', 'embedding'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('qa_outputs', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'encoder_attn_layer_norm', 'kernel'), ('model', 'encoder', 'embed_tokens', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '1', 'final_layer_norm', 'bias'), ('encoder', 'embed_positions', 'kernel'), ('model', 'encoder', 'layers', '1', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'fc2', 'kernel'), ('encoder', 'layers', '1', 'fc1', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'fc1', 'kernel'), ('encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'fc1', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layernorm_embedding', 'bias'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '0', 'fc2', 'kernel'), ('classification_head', 'dense', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'fc2', 'bias'), ('model', 'encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('model', 'decoder', 'layers', '0', 'final_layer_norm', 'bias'), ('encoder', 'layers', '0', 'fc2', 'bias'), ('model', 'decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'bias'), ('model', 'encoder', 'layers', '1', 'fc2', 'kernel'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'encoder_attn_layer_norm', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'fc1', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layernorm_embedding', 'bias'), ('decoder', 'layers', '1', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('lm_head', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc2', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '0', 'fc1', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('model', 'decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'fc2', 'bias'), ('classification_head', 'out_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('model', 'decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'fc2', 'bias'), ('encoder', 'layers', '1', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'bias'), ('model', 'encoder', 'embed_positions', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBartForCausalLM were not initialized from the model checkpoint at hf-internal-testing/tiny-random-bart and are newly initialized: {('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'fc1', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'fc2', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'fc1', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'fc2', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'encoder_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', 'FlaxBartDecoderLayers', 'self_attn', 'k_proj', 'bias')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder checkpointing: False\n",
      "encoder scan: True\n",
      "decoder checkpointing: False\n",
      "decoder scan: True\n"
     ]
    }
   ],
   "source": [
    "custom_model = CustomFlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_use_scan=True, decoder_use_scan=True, encoder_from_pt=True, decoder_from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3eb161b-f1d2-4b5d-a616-caa1b82b4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_params = hf_model.params\n",
    "custom_params = custom_model.params\n",
    "custom_params = flatten_dict(custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "683015fb-cd4e-4472-8149-1e67e6fc95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unrolled_to_scanned(vs):\n",
    "    vs = unfreeze(vs)\n",
    "    Ws = jnp.stack([vs['params'][f'SinDot_{i}']['W'] for i in range(L)])\n",
    "    bs = jnp.stack([vs['params'][f'SinDot_{i}']['b'] for i in range(L)])\n",
    "    new_vs = {'params': {}}\n",
    "    new_vs['params']['scanned_layer'] = {}\n",
    "    new_vs['params']['scanned_layer']['W'] = Ws\n",
    "    new_vs['params']['scanned_layer']['b'] = bs\n",
    "    return freeze(new_vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa404cd1-c8ae-4067-bba0-c53c46fb9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unrolled_to_scanned(hf_params):\n",
    "    new_enc_params = {}\n",
    "    # get the key of a scanned module\n",
    "    for k in flatten_dict(hf_params['encoder']['encoder']['layers']['0']):\n",
    "        # stack the weights for each layer of the scanned module into one matrix\n",
    "        new_enc_params[k] = jnp.stack([flatten_dict(hf_params['encoder']['encoder']['layers'][str(i)])[k] for i in range(num_encoder_layers)])\n",
    "    # append the correct prefix to the scanned modules' keys\n",
    "    new_enc_params = unflatten_dict({('encoder', 'layers', 'FlaxWav2Vec2EncoderLayers'): unflatten_dict(new_enc_params)})\n",
    "    \n",
    "    # repeat for the decoder (note that the key 'layers' appears one index to the right than in the encoder, thus we'll treat the encoder and decoder independently for now)\n",
    "    new_dec_params = {}\n",
    "    for k in flatten_dict(hf_params['decoder']['model']['decoder']['layers']['0']):\n",
    "        new_dec_params[k] = jnp.stack([flatten_dict(hf_params['decoder']['model']['decoder']['layers'][str(i)])[k] for i in range(num_decoder_layers)])\n",
    "    new_dec_params = unflatten_dict({('model', 'decoder', 'layers', 'FlaxBartDecoderLayers'): unflatten_dict(new_dec_params)})\n",
    "    \n",
    "    # combine the encoder and decoder parameters\n",
    "    new_params = {'encoder': new_enc_params, 'decoder': new_dec_params}\n",
    "    new_params = flatten_dict(new_params)\n",
    "    \n",
    "    # append parameters for non-scanned modules (i.e. all modules that do not contain the key 'layers')\n",
    "    for k in flatten_dict(hf_params):\n",
    "        if 'layers' not in k:\n",
    "            new_params[k] = flatten_dict(hf_params)[k]\n",
    "\n",
    "    return unflatten_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5bc7eed-e5df-46a6-96ba-d19edc747906",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.params = unrolled_to_scanned(hf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15307c0c-ed5f-4c70-8089-67b0180e5a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64 0\n"
     ]
    }
   ],
   "source": [
    "match = []\n",
    "mismatch = []\n",
    "\n",
    "flat_hf_params = flatten_dict(hf_params['encoder']['encoder']['layers'])\n",
    "flat_custom_params = flatten_dict(custom_model.params['encoder']['encoder']['layers']['FlaxWav2Vec2EncoderLayers'])\n",
    "\n",
    "for k in flat_hf_params:\n",
    "    assert flat_custom_params[k[1:]][int(k[0])].shape == flat_hf_params[k].shape, \"shapes do not match\"\n",
    "    \n",
    "    if (flat_custom_params[k[1:]][int(k[0])] == flat_hf_params[k]).all():\n",
    "        match.append(k)\n",
    "        \n",
    "    if (flat_custom_params[k[1:]][int(k[0])] != flat_hf_params[k]).all():\n",
    "        mismatch.append(k)\n",
    "        \n",
    "print(len(match) + len(mismatch), len(match), len(mismatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3c5f88-8991-4a09-bcaf-e62f8d71f50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52 0\n"
     ]
    }
   ],
   "source": [
    "match = []\n",
    "mismatch = []\n",
    "\n",
    "flat_hf_params = flatten_dict(hf_params['decoder']['model']['decoder']['layers'])\n",
    "flat_custom_params = flatten_dict(custom_model.params['decoder']['model']['decoder']['layers']['FlaxBartDecoderLayers'])\n",
    "\n",
    "for k in flat_hf_params:\n",
    "    assert flat_custom_params[k[1:]][int(k[0])].shape == flat_hf_params[k].shape, \"shapes do not match\"\n",
    "    \n",
    "    if (flat_custom_params[k[1:]][int(k[0])] == flat_hf_params[k]).all():\n",
    "        match.append(k)\n",
    "        \n",
    "    if (flat_custom_params[k[1:]][int(k[0])] != flat_hf_params[k]).all():\n",
    "        mismatch.append(k)\n",
    "        \n",
    "print(len(match) + len(mismatch), len(match), len(mismatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a869343-21cf-4711-857b-29f14ad68b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some dummy data\n",
    "inputs = np.random.randn(1, 5000)\n",
    "decoder_input_ids = np.arange(100).reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a884387c-783a-49f9-9e07-d8aff6470c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ground-truth outputs from Transformers ðŸ¤— model\n",
    "hf_outputs = hf_model(inputs, decoder_input_ids=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "960fe775-64aa-4202-a3c0-44dcdb4f6523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder checkpointing: False\n",
      "encoder scan: True\n",
      "decoder checkpointing: False\n",
      "decoder scan: False\n"
     ]
    }
   ],
   "source": [
    "custom_outputs = custom_model(inputs, decoder_input_ids=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c14df64-ffa1-478c-a439-bfab7122c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function for our analysis\n",
    "def assert_almost_equals(a: np.ndarray, b: np.ndarray, tol: float = 1e-5):\n",
    "    diff = np.abs((a - b)).max()\n",
    "    if diff <= tol:\n",
    "        print(f\"âœ… Difference between HF and custom is {diff} (< {tol})\")\n",
    "    else:\n",
    "        print(f\"âŒ Difference between HF and custom is {diff} (>= {tol})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81e39f69-aae0-4f7a-8061-8dc5556b7c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking encoder last hidden states match--------------------------\n",
      "HF output shape: (2, 76, 16), custom output shape: (2, 76, 16)\n",
      "âœ… Difference between HF and custom is 3.5762786865234375e-07 (< 1e-05)\n",
      "--------------------------Checking logits match--------------------------\n",
      "HF logits shape: (2, 50, 1000), Custom logits shape: (2, 50, 1000)\n",
      "âœ… Difference between HF and custom is 8.940696716308594e-08 (< 1e-05)\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking encoder last hidden states match--------------------------\")\n",
    "print(f\"HF output shape: {hf_outputs.encoder_last_hidden_state.shape}, custom output shape: {custom_outputs.encoder_last_hidden_state.shape}\")\n",
    "assert_almost_equals(hf_outputs.encoder_last_hidden_state, custom_outputs.encoder_last_hidden_state)\n",
    "\n",
    "print(\"--------------------------Checking logits match--------------------------\")\n",
    "print(f\"HF logits shape: {hf_outputs.logits.shape}, Custom logits shape: {custom_outputs.logits.shape}\")\n",
    "assert_almost_equals(hf_outputs.logits, custom_outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613867d8-09af-42a8-8c50-c04ddf498358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
